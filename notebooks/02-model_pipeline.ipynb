{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUTFwSh5u9OK"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsLff0QbdE8D"
   },
   "source": [
    "# Build & Experiment using Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0C7kTlrH1bO8"
   },
   "source": [
    "This notebook showcases an end-to-end Machine Learning Operations (MLOps) pipeline built with Vertex AI, leveraging the Ames Iowa Housing data. It serves as a pattern for rapid experimentation using notebooks and demonstrates how to develop models within a structured, automated workflow\n",
    "\n",
    "## Flow\n",
    "![housing-pipeline.png](./diagrams/housing-pipeline.png)\n",
    "\n",
    "\n",
    "Documentation: https://cloud.google.com/vertex-ai/docs/pipelines/introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RakMIliNYh8O"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSHmJT9cTggu"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "936Zz5YI2NeA"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66oJ55lG2Tiq"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules:\n",
    "\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Clr61ben2WwY"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v848aGbn2acH"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVeoyQPz2cfh"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules:\n",
    "\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    pipeline,\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HId-ySlY2jlI"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
    "\n",
    "- `PROJECT_ID`: Google Cloud project ID where Vertex AI resources are deployed\n",
    "- `LOCATION`: Google Cloud region where the Vertex AI endpoint is located\n",
    "- `VERSION`: Version tag for Docker serving container image\n",
    "- `REPO_NAME`: Artifact Registry repository name.\n",
    "- `JOB_IMAGE_ID`: Docker image name for custom jobs.\n",
    "- `BUCKET_URI`: Google Cloud Storage bucket URI to store model artifacts and other data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4gnZI9OX6VJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"sandbox-401718\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "VERSION=\"latest\" \n",
    "REPO_NAME=\"housing-poc\" # @param {type:\"string\"}\n",
    "JOB_IMAGE_ID=\"housing-poc-image\" # @param {type:\"string\"}\n",
    "\n",
    "# Create GCS Bucket\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-pred-benchmark\"  # @param {type:\"string\"}\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NrpFROTjoVL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Components\n",
    "\n",
    "Components are self-contained, containerized tasks that operate as isolated jobs, each with its own environment and explicit interfaces\n",
    "\n",
    "- `Import Data Source:`\n",
    "This initial component represents the pipeline's entry point where raw data, like the Ames Iowa Housing dataset, is ingested. It acts as a placeholder for various data sources, ensuring the pipeline can be fed with the necessary information to begin processing.\n",
    "\n",
    "- `Feature Transformation:`\n",
    "Following data import, this crucial step processes and prepares the raw data for model training. It performs tasks such as data cleaning, feature engineering, and splitting the dataset into training and testing subsets (X_train, y_train, X_test, y_test).\n",
    "\n",
    "- `X_test_dataset / X_train_dataset / y_test_dataset / y_train_dataset:`\n",
    "These represent the intermediate outputs of the Feature Transformation step. They are structured datasets (system.Dataset) containing the partitioned feature (X) and target (y) sets for both training and testing, ensuring data readiness for the subsequent model training phase.\n",
    "\n",
    "- `Model Training:`\n",
    "This core component takes the prepared training and testing datasets to build and validate a machine learning model. It typically involves fitting an algorithm to the training data and evaluating its performance before producing a trained model.\n",
    "\n",
    "- `pickled_model:`\n",
    "This output artifact from the Model Training component represents the fully trained model in a serialized (pickled) format. It's the tangible result of the training process, ready to be registered or deployed for inference.\n",
    "\n",
    "- `Register Model to Vertex:`\n",
    "As the final step in this pipeline, this component takes the trained model and integrates it with the Vertex AI Model Registry. It formalizes the model for MLOps, allowing for versioning, centralized management, and subsequent deployment to prediction endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Import\n",
    "\n",
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-6:latest\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def import_data_op():\n",
    "    print(\"Dummy operator to import data from BQ / Cloud Storage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "\n",
    "from kfp.dsl import Output, Dataset, component\n",
    "\n",
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-6:latest\",\n",
    "    packages_to_install=[\n",
    "        \"fsspec\",\n",
    "        \"gcsfs\",\n",
    "        \"pandas\",\n",
    "    ],\n",
    ")\n",
    "def transformation_op(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    X_train_dataset: Output[Dataset],\n",
    "    X_test_dataset: Output[Dataset],\n",
    "    y_train_dataset: Output[Dataset],\n",
    "    y_test_dataset: Output[Dataset],\n",
    "    current_year: int = 2024,\n",
    "    test_split_ratio: float = 0.1,\n",
    "    random_state_seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads Ames housing data, preprocesses features, splits into train/test sets,\n",
    "    and saves them as separate Dataset artifacts (.npy format).\n",
    "    \"\"\"\n",
    "    # === Imports inside the function ===\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # === Data Loading ===\n",
    "    data_url = \"https://raw.githubusercontent.com/melindaleung/Ames-Iowa-Housing-Dataset/master/data/ames%20iowa%20housing.csv\"\n",
    "    print(f\"Loading data from: {data_url}\")\n",
    "    try:\n",
    "        df = pd.read_csv(data_url)\n",
    "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load data: {e}\")\n",
    "        raise  # Re-raise the exception to fail the component\n",
    "\n",
    "    # === Feature Selection ===\n",
    "    features_to_keep = [\"LotFrontage\", \"LotArea\", \"YearBuilt\", \"SaleType\", \"SalePrice\"]\n",
    "    print(f\"Selecting features: {features_to_keep}\")\n",
    "\n",
    "    df = df[features_to_keep].copy()\n",
    "\n",
    "    # === Preprocessing ===\n",
    "    print(\"Starting preprocessing...\")\n",
    "    # Impute missing LotFrontage with the mean\n",
    "\n",
    "    lot_frontage_mean = df[\"LotFrontage\"].mean()\n",
    "    df[\"LotFrontage\"].fillna(lot_frontage_mean, inplace=True)\n",
    "    print(f\"Imputed 'LotFrontage' NaN with mean: {lot_frontage_mean:.2f}\")\n",
    "\n",
    "    # Log transform LotArea\n",
    "\n",
    "    df[\"LotArea\"] = np.log1p(df[\"LotArea\"])\n",
    "    print(\"Applied log1p transformation to 'LotArea'\")\n",
    "\n",
    "    # Create HouseAge feature\n",
    "\n",
    "    df[\"HouseAge\"] = current_year - df[\"YearBuilt\"]\n",
    "    df.drop(\"YearBuilt\", axis=1, inplace=True)\n",
    "    print(\n",
    "        f\"Created 'HouseAge' using current_year={current_year} and dropped 'YearBuilt'\"\n",
    "    )\n",
    "\n",
    "    # Label Encode SaleType\n",
    "\n",
    "    if \"SaleType\" in df.columns:\n",
    "        df[\"SaleType\"] = df[\"SaleType\"].astype(\n",
    "            str\n",
    "        )  # Convert to string to handle potential NaN/mixed types robustly\n",
    "        label_encoder = (\n",
    "            LabelEncoder()\n",
    "        )  ##################################################\n",
    "        df[\"SaleType\"] = label_encoder.fit_transform(df[\"SaleType\"])\n",
    "        print(\"Label encoded 'SaleType'\")\n",
    "    else:\n",
    "        print(\"Warning: 'SaleType' column not found. Skipping encoding.\")\n",
    "\n",
    "    # === Define Features X and Target y ===\n",
    "\n",
    "    target_column = \"SalePrice\"\n",
    "    feature_columns = [\"LotFrontage\", \"LotArea\", \"HouseAge\", \"SaleType\"]\n",
    "\n",
    "    # Ensure all expected feature columns exist after preprocessing\n",
    "\n",
    "    missing_features = [col for col in feature_columns if col not in df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(\n",
    "            f\"Missing expected feature columns after preprocessing: {missing_features}\"\n",
    "        )\n",
    "    print(f\"Defining features (X) from columns: {feature_columns}\")\n",
    "    print(f\"Defining target (y) from column: {target_column}\")\n",
    "    X = df[feature_columns].values  # Convert features to numpy array\n",
    "    y = df[target_column].values  # Convert target to numpy array\n",
    "\n",
    "    # === Split into Training and Testing Sets ===\n",
    "    print(\n",
    "        f\"Splitting data with test_size={test_split_ratio} and random_state={random_state_seed}\"\n",
    "    )\n",
    "    X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "        X, y, test_size=test_split_ratio, random_state=random_state_seed\n",
    "    )\n",
    "\n",
    "    # === Save Outputs to KFP Provided Paths ===\n",
    "\n",
    "    print(f\"Saving X_train to: {X_train_dataset.path}\")\n",
    "    np.save(X_train_dataset.path, X_train_np)\n",
    "    # Add metadata (optional but good practice)\n",
    "\n",
    "    X_train_dataset.metadata[\"npy_shape\"] = X_train_np.shape\n",
    "    X_train_dataset.metadata[\"description\"] = \"Training features\"\n",
    "\n",
    "    print(f\"Saving X_test to: {X_test_dataset.path}\")\n",
    "    np.save(X_test_dataset.path, X_test_np)\n",
    "    X_test_dataset.metadata[\"npy_shape\"] = X_test_np.shape\n",
    "    X_test_dataset.metadata[\"description\"] = \"Test features\"\n",
    "\n",
    "    print(f\"Saving y_train to: {y_train_dataset.path}\")\n",
    "    np.save(y_train_dataset.path, y_train_np)\n",
    "    y_train_dataset.metadata[\"npy_shape\"] = y_train_np.shape\n",
    "    y_train_dataset.metadata[\"description\"] = \"Training target\"\n",
    "\n",
    "    print(f\"Saving y_test to: {y_test_dataset.path}\")\n",
    "    np.save(y_test_dataset.path, y_test_np)\n",
    "    y_test_dataset.metadata[\"npy_shape\"] = y_test_np.shape\n",
    "    y_test_dataset.metadata[\"description\"] = \"Test target\"\n",
    "\n",
    "    print(\"Transformation component finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "TrainingOutputs = NamedTuple(\n",
    "    \"TrainingOutputs\",\n",
    "    [\n",
    "        (\"model_gcs_path\", str), # GCS path\n",
    "        (\"pickled_model_output\", Output[Model]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-6:latest\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"scikit-learn\", \"numpy\"],\n",
    ")\n",
    "def training_op(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    BUCKET_URI: str,\n",
    "    X_train_dataset: Input[Dataset],\n",
    "    X_test_dataset: Input[Dataset],\n",
    "    y_train_dataset: Input[Dataset],\n",
    "    y_test_dataset: Input[Dataset],\n",
    "    pickled_model: Output[Model],\n",
    "    filename: str = 'house_price_model.pkl',\n",
    ") -> NamedTuple(\"Outputs\", [(\"model_gcs_path\", str),]):\n",
    "\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from google.cloud import storage\n",
    "    from collections import namedtuple\n",
    "\n",
    "    print(f\"Loading X_train from base path: {X_train_dataset.path}\")\n",
    "    X_train = np.load(X_train_dataset.path + \".npy\") # Append .npy\n",
    "\n",
    "    print(f\"Loading X_test from base path: {X_test_dataset.path}\")\n",
    "    X_test = np.load(X_test_dataset.path + \".npy\")   # Append .npy\n",
    "\n",
    "    print(f\"Loading y_train from base path: {y_train_dataset.path}\")\n",
    "    y_train = np.load(y_train_dataset.path + \".npy\") # Append .npy\n",
    "\n",
    "    print(f\"Loading y_test from base path: {y_test_dataset.path}\")\n",
    "    y_test = np.load(y_test_dataset.path + \".npy\")   # Append .npy\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if X_test.shape[0] > 0:\n",
    "        predictions = model.predict(X_test[:1])\n",
    "        print(f\"Sample prediction on first test instance: {predictions}\")\n",
    "    else:\n",
    "        print(\"Test set is empty, skipping sample prediction.\")\n",
    "\n",
    "    with open(pickled_model.path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model pickled and saved to KFP managed path: {pickled_model.path}\")\n",
    "    pickled_model.metadata[\"framework\"] = \"scikit-learn\"\n",
    "    pickled_model.metadata[\"model_type\"] = \"LinearRegression\"\n",
    "    pickled_model.metadata[\"description\"] = \"House price prediction model.\"\n",
    "\n",
    "    local_upload_filename = filename\n",
    "    with open(local_upload_filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    gcs_model_path_in_bucket = f\"housing_models/{filename}\"\n",
    "    bucket_name = BUCKET_URI.replace(\"gs://\", \"\").split('/')[0]\n",
    "\n",
    "    print(f\"Uploading {local_upload_filename} to gs://{bucket_name}/{gcs_model_path_in_bucket}\")\n",
    "    storage_client = storage.Client(project=project if project else None)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(gcs_model_path_in_bucket)\n",
    "    blob.upload_from_filename(local_upload_filename)\n",
    "\n",
    "    uploaded_model_gcs_uri = f\"gs://{bucket_name}/{gcs_model_path_in_bucket}\"\n",
    "    print(f\"Model additionally uploaded to: {uploaded_model_gcs_uri}\")\n",
    "\n",
    "    Outputs = namedtuple(\"Outputs\", [\"model_gcs_path\"])\n",
    "    return Outputs(model_gcs_path=uploaded_model_gcs_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register Model\n",
    "\n",
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-6:latest\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"fsspec\", \"gcsfs\"], # fsspec, gcsfs can be helpful for GCS path handling by aiplatform SDK\n",
    ")\n",
    "def model_registry_op(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_gcs_uri: str,\n",
    "    serving_container_image: str, # Full URI of the serving container\n",
    "    model_display_name: str = \"housing-model-registered\", # Default display name, can be overridden\n",
    ") -> NamedTuple(\"RegistryOutputs\", [(\"model_resource_name\", str), (\"model_version_id\", str)]):\n",
    "    \"\"\"\n",
    "    Uploads a trained model to Vertex AI Model Registry.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from google.cloud import aiplatform\n",
    "    from collections import namedtuple\n",
    "\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    # The artifact_uri for Model.upload should be the GCS *directory*\n",
    "    # containing the model artifact. model_gcs_uri is the path to the file itself.\n",
    "    upload_artifact_uri = os.path.dirname(model_gcs_uri)\n",
    "\n",
    "    # aiplatform.Model.upload will create a new model or a new version\n",
    "    uploaded_model = aiplatform.Model.upload(\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=upload_artifact_uri, # GCS directory containing the model\n",
    "        serving_container_image_uri=serving_container_image,\n",
    "        serving_container_predict_route=\"/predict\", \n",
    "        serving_container_health_route=\"/health\",   \n",
    "        serving_container_ports=[8080],             \n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    RegistryOutputs = namedtuple(\"RegistryOutputs\", [\"model_resource_name\", \"model_version_id\"])\n",
    "    return RegistryOutputs(\n",
    "        model_resource_name=uploaded_model.resource_name,\n",
    "        model_version_id=uploaded_model.version_id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(pipeline_root=BUCKET_URI, name=\"passage-gen-example\")\n",
    "def pipeline():\n",
    "\n",
    "    import_data = import_data_op().set_display_name(\"Import Data Source\")\n",
    "\n",
    "    transformation = transformation_op(\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "    ).after(import_data).set_display_name(\"Feature Transformation\")\n",
    "\n",
    "    training = training_op(\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "        BUCKET_URI=BUCKET_URI,\n",
    "        X_train_dataset=transformation.outputs[\"X_train_dataset\"],\n",
    "        X_test_dataset=transformation.outputs[\"X_test_dataset\"],\n",
    "        y_train_dataset=transformation.outputs[\"y_train_dataset\"],\n",
    "        y_test_dataset=transformation.outputs[\"y_test_dataset\"],\n",
    "    ).after(transformation).set_display_name(\"Model Training\")\n",
    "\n",
    "    model_registry = model_registry_op(\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "        model_gcs_uri=training.outputs[\"model_gcs_path\"],\n",
    "        serving_container_image=f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{JOB_IMAGE_ID}:{VERSION}\",\n",
    "        model_display_name=\"housing_model\"\n",
    "    ).after(training).set_display_name(\"Register Model to Vertex AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pipeline\n",
    "\n",
    "Vertex AI Pipelines lets you automate, monitor, and govern your ML systems in a serverless manner by using ML pipelines to orchestrate your ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_uri = 'pipeline.yaml'\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=template_uri,\n",
    ")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    job_id=f\"housing-pipeline-{UUID}\",\n",
    "    display_name=\"housing-model-poc\",\n",
    "    pipeline_root=os.path.join(BUCKET_URI),\n",
    "    template_path=template_uri,\n",
    "    enable_caching=False,\n",
    ")\n",
    "job.run(sync=False) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_psc_private_endpoint.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
